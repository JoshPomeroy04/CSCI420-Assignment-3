# CSCI420-Assignment-3

# **1. Introduction**  
This project explores **prompting techniques**. In this project, Gemini and GPT are used to explore the effectiveness of different prompting techniques. The techniques used include: zero-shot, few-shot, chain-of-thoughts, role-playing, and prompt chaining. Results are compared using BLEU and CodeBLEU metrics.

# **2. Getting Started**  

The compare script is implemented in **Python 3.10.12** 64-bit. It was developed and tested on **Ubuntu 22.04.3 LTS**.  

The models were accessed through their online websites. \
GPT-4o: https://chatgpt.com \
Gemini 2.0 Flash: https://gemini.google.com/app

## **2.1 Preparations**  

(1) Clone the repository to your workspace:  
```shell
~ $ git clone https://github.com/JoshPomeroy04/CSCI420-Assignment-3.git
```
(2) Navigate into the repository



Ensure you set the FOLDER_PATH variable to be the correct folder path of the workspace. This variable is easily found at the top of compare.py

## **2.2 Install Packages**

Install the required dependencies: \
python -m pip install tree-sitter==0.23 \
python -m pip install tree-sitter-python \
python -m pip install sacrebleu \
python -m pip install csv \
python -m pip install codebleu[all]


## **2.3 Evaluate**

The results of the BLEU and CodeBLEU metrics can be found in the results folder. Inside are 4 files. gemini.csv contains the scores of checking Gemini generated responses. It compares zero-shot prompting to a secondary prompting technique. gpt.csv is the same, the only difference is that the responses were generated by GPT. gpt-gemini-R1.csv contains the scores from checking the zero-shot responses from gpt against those of gemini. gpt-gemini-R2.csv has the scores from checking the secondary technique responses from gpt against those of gemini. The raw prompts can be found in Gemini Prompts and GPT Prompts. The responses used to calculate scores can be found in data. 

